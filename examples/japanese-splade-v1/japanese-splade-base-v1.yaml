bf16: true
dataloader_drop_last: true
dataloader_num_workers: 12
gradient_accumulation_steps: 8
per_device_train_batch_size: 4
learning_rate: 2.0e-05
logging_steps: 200
lr_scheduler_type: cosine
max_grad_norm: 1.0
max_length: 512
model_name_or_path: tohoku-nlp/bert-base-japanese-v3
noise_tokens: "\" \u3020 ! # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` {\
  \ | } ~ \xA1 \xA2 \xA3 \xA4 \xA5 \xA6 \xA7 \xA9 \xAB \xAC \xAE \xB0 \xB1 \xB6 \xB7\
  \ \xBB \xBF \xC5 \xD7 \xF7 \u0127 \u0429 \u0449 \u044A \u05D0 \u0E34 \u10DA \u2010\
  \ \u2013 \u2014 \u2015 \u2016 \u2020 \u2021 \u2022 \u2032 \u203B \u203F \u2042 \u2051\
  \ \u20AC \u2127 \u2190 \u2191 \u2192 \u2193 \u2194 \u2196 \u2197 \u2198 \u2199 \u21C4\
  \ \u21D2 \u21D4 \u21E6 \u21E7 \u21E8 \u21E9 \u2200 \u2202 \u2203 \u2205 \u2207 \u2208\
  \ \u2209 \u220B \u2212 \u2213 \u221A \u221D \u221E \u221F \u2220 \u2225 \u2226 \u2227\
  \ \u2228 \u2229 \u222A \u222B \u222E \u2234 \u2235 \u223D \u2243 \u2245 \u2248 \u2252\
  \ \u2260 \u2261 \u2262 \u2266 \u2267 \u226A \u226B \u2276 \u2277 \u2282 \u2283 \u2284\
  \ \u2285 \u2286 \u2287 \u228A \u228B \u2295 \u2296 \u2297 \u22A5 \u22BF \u22DA \u22DB\
  \ \u2305 \u2306 \u2312 \u2318 \u23BE \u23BF \u23C0 \u23C1 \u23C2 \u23C3 \u23C4 \u23C5\
  \ \u23C6 \u23C7 \u23C8 \u23C9 \u23CA \u23CB \u23CC \u23CE \u24EB \u24EC \u24ED \u24EE\
  \ \u24EF \u24F0 \u24F1 \u24F2 \u24F3 \u24F4 \u24F5 \u24F6 \u24F7 \u24F8 \u24F9 \u24FA\
  \ \u24FB \u24FC \u24FD \u24FE \u2500 \u2501 \u250C \u2510 \u2518 \u251C \u2579 \u25A0\
  \ \u25A1 \u25B1 \u25B2 \u25B3 \u25B6 \u25B7 \u25BC \u25BD \u25C0 \u25C1 \u25C6 \u25C7\
  \ \u25C9 \u25CB \u25CE \u25CF \u25D0 \u25D1 \u25D2 \u25D3 \u25E1 \u25E6 \u25EF \u2600\
  \ \u2601 \u2602 \u2603 \u2605 \u2606 \u260E \u2616 \u2617 \u261E \u2640 \u2642 \u2660\
  \ \u2661 \u2662 \u2663 \u2664 \u2665 \u2666 \u2667 \u2668 \u2669 \u266A \u266B \u266C\
  \ \u266D \u266E \u266F \u2713 \u2756 \u2776 \u2777 \u2778 \u2779 \u277A \u277B \u277C\
  \ \u277D \u277E \u277F \u2934 \u2935 \u2985 \u2986 \u29BF \u29FA \u29FB \u3001 \u3002\
  \ \u3003 \u3005 \u3007 \u3008 \u3009 \u300A \u300B \u300C \u300D \u300E \u300F \u3010\
  \ \u3011 \u3012 \u3013 \u3014 \u3015 \u3016 \u3017 \u3018 \u3019 \u301C \u301D \u301F\
  \ \u3020 \u3033 \u3034 \u3035 \u303B \u303D \u3041 \u3043 \u3045 \u3047 \u3049 \u3063\
  \ \u309D \u309E \u30A0 \u30A1 \u30A5 \u30A7 \u30A9 \u30C3 \u30FB \u30FC \u30FD \u30FE\
  \ \u4E3F \u4EDD \u5C6E \u5F61 \uFE45 \uFE46 ]\u3001"
num_train_epochs: 2
output_dir: ./output/japanese-splade-base-v1
overwrite_output_dir: true
regularizer_doc: L1
regularizer_query: L1
remove_checkpoints: true
run_name: japanese-splade-base-v1
save_steps: 5000
save_total_limit: 2
seed: 42
sparsity_warmup_steps_doc: 0.1
sparsity_warmup_steps_query: 0.1
sparsity_weight_doc: 0.001
sparsity_weight_query: 0.0025
train_data:
- dataset_class: yast.custom_dataset.mmarco.MMarcoHardNegatives
  train_data:
    lang: english
    reranker: bge-reranker-v2-m3
- dataset_class: yast.custom_dataset.hpprc_emb_scores.HpprcEmbScoresDataset
  train_data:
  - subset: auto-wiki-qa
  - subset: mmarco
  - subset: jsquad
  - subset: jaquad
  - subset: auto-wiki-qa-nemotron
  - subset: quiz-works
  - subset: quiz-no-mori
  - aug_factor: 5
    subset: miracl
  - aug_factor: 8
    subset: jqara
  - aug_factor: 5
    subset: mr-tydi
  - aug_factor: 3
    subset: baobab-wiki-retrieval
  - subset: mkqa
train_group_size: 8
training_losses: cross_entropy
warmup_ratio: 0.05
weight_decay: 0
